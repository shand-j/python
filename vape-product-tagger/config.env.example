# Vape Product Tagger Configuration

# ==================== Ollama AI Configuration ====================
# Ollama service URL (local or remote). OLLAMA_HOST also supported.
OLLAMA_BASE_URL=http://localhost:11434
# Explicit host override (optional)
OLLAMA_HOST=

# Ollama model to use (llama2, mistral, etc.)
OLLAMA_MODEL=llama3.1:latest

# Ollama request timeout in seconds
OLLAMA_TIMEOUT=180

# Maximum concurrent requests the Ollama daemon will process (match `ollama serve --num-parallel`)
OLLAMA_NUM_PARALLEL=6

# Keep models warm for faster subsequent calls (passed to `ollama serve --keep-alive`)
OLLAMA_KEEP_ALIVE=5m

# Optional secondary model for rescue tagging
OLLAMA_SECONDARY_MODEL=mistral:latest

# Optional tertiary model for final fallback  
OLLAMA_TERTIARY_MODEL=llama3.1:latest

# Enable AI cascade (primary → secondary → tertiary on low confidence)
ENABLE_AI_CASCADE=true

# Sliding window (seconds) used to smooth progress rate logs
PROGRESS_RATE_WINDOW_SECONDS=30

# ==================== AI Cascade Configuration ====================
# Primary AI model for tagging (first attempt)
PRIMARY_AI_MODEL=mistral:latest

# Secondary AI model (fallback if primary fails or low confidence)
SECONDARY_AI_MODEL=gpt-oss:latest

# Tertiary AI model (last resort fallback)
TERTIARY_AI_MODEL=llama3.1:latest

# AI confidence threshold (0.0 to 1.0) - attempts cascade if below this
AI_CONFIDENCE_THRESHOLD=0.7

# Enable third opinion recovery for failed validations (true/false)
ENABLE_THIRD_OPINION=true

# ==================== AI Processing Configuration ====================
# Enable AI-powered tagging (true/false)
ENABLE_AI_TAGGING=true

# Cache AI-generated tags to improve performance (true/false)
CACHE_AI_TAGS=true

# ==================== Batch Processing Configuration ====================
# Number of products to process in a batch
BATCH_SIZE=8

# Enable parallel processing (true/false)
PARALLEL_PROCESSING=true

# Maximum number of worker threads
MAX_WORKERS=6

# ==================== Output Configuration ====================
# Output directory for exported files
OUTPUT_DIR=./output

# Logs directory
LOGS_DIR=./logs

# Cache directory for AI tags
CACHE_DIR=./cache

# Default output format (csv or json)
OUTPUT_FORMAT=csv

# ==================== Shopify Configuration ====================
# Default vendor name for products
SHOPIFY_VENDOR=Vapourism

# Default product type
SHOPIFY_PRODUCT_TYPE=Vaping and CBD Products

# Auto-publish products (true/false)
AUTO_PUBLISH=false

# ==================== Collection Generation Configuration ====================
# Automatically generate collections based on tags (true/false)
AUTO_GENERATE_COLLECTIONS=true

# Prefix for generated collection names (optional)
COLLECTION_PREFIX=

# ==================== Compliance Configuration ====================
# Enable compliance tagging (true/false) - Note: Compliance tags not in approved_tags.json
ENABLE_COMPLIANCE_TAGS=false

# Default age restriction
DEFAULT_AGE_RESTRICTION=18+

# Regional compliance (comma-separated: US, EU, etc.)
REGIONAL_COMPLIANCE=UK

# ==================== Pipeline Configuration ====================
# Pipeline execution mode: local or vastai
PIPELINE_MODE=local

# Auto-launch review interface after tagging (true/false)
AUTO_REVIEW_INTERFACE=false

# Automatically export training data after run (true/false)
TRAINING_DATA_AUTO_EXPORT=true

# ==================== Logging Configuration ====================
# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Enable verbose logging (true/false)
VERBOSE_LOGGING=true

# ==================== Model Backend Configuration ====================
# Model backend: ollama (local) or huggingface (cloud/fine-tuned)
MODEL_BACKEND=ollama

# ==================== Hugging Face Hub Configuration ====================
# HF access token (required for gated models like Llama 3.1)
HF_TOKEN=

# HF repo for pushing/pulling LoRA adapters
HF_REPO_ID=coglabs-john/product_tagger

# ==================== Training Configuration (Vast.ai) ====================
# Base model for fine-tuning
BASE_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# Quantization bits (4 or 8)
QUANTIZATION_BITS=4

# LoRA hyperparameters
LORA_R=64
LORA_ALPHA=128
LORA_DROPOUT=0.05

# Training settings
TRAIN_VAL_SPLIT=0.8
MAX_SEQ_LENGTH=2048
LEARNING_RATE=2e-4
WARMUP_RATIO=0.03
GRADIENT_ACCUMULATION_STEPS=4