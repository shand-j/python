# Vape Product Tagger Configuration

# ==================== Ollama AI Configuration ====================
# Ollama service URL (local or remote)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use (llama2, mistral, etc.)
OLLAMA_MODEL=llama2

# Ollama request timeout in seconds
OLLAMA_TIMEOUT=60

# ==================== AI Processing Configuration ====================
# Enable AI-powered tagging (true/false)
ENABLE_AI_TAGGING=true

# AI confidence threshold (0.0 to 1.0)
AI_CONFIDENCE_THRESHOLD=0.7

# Cache AI-generated tags to improve performance (true/false)
CACHE_AI_TAGS=true

# ==================== Batch Processing Configuration ====================
# Number of products to process in a batch
BATCH_SIZE=10

# Enable parallel processing (true/false)
PARALLEL_PROCESSING=true

# Maximum number of worker threads
MAX_WORKERS=4

# ==================== Output Configuration ====================
# Output directory for exported files
OUTPUT_DIR=./output

# Logs directory
LOGS_DIR=./logs

# Cache directory for AI tags
CACHE_DIR=./cache

# Default output format (csv or json)
OUTPUT_FORMAT=csv

# ==================== Shopify Configuration ====================
# Default vendor name for products
SHOPIFY_VENDOR=Vape Store

# Default product type
SHOPIFY_PRODUCT_TYPE=Vaping Products

# Auto-publish products (true/false)
AUTO_PUBLISH=false

# ==================== Collection Generation Configuration ====================
# Automatically generate collections based on tags (true/false)
AUTO_GENERATE_COLLECTIONS=true

# Prefix for generated collection names (optional)
COLLECTION_PREFIX=

# ==================== Compliance Configuration ====================
# Enable compliance tagging (true/false)
ENABLE_COMPLIANCE_TAGS=true

# Default age restriction
DEFAULT_AGE_RESTRICTION=18+

# Regional compliance (comma-separated: US, EU, etc.)
REGIONAL_COMPLIANCE=US

# ==================== Logging Configuration ====================
# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Enable verbose logging (true/false)
VERBOSE_LOGGING=false

# ==================== Model Backend Configuration ====================
# Model backend: ollama (local) or huggingface (cloud/fine-tuned)
MODEL_BACKEND=ollama

# ==================== Hugging Face Hub Configuration ====================
# HF access token (required for gated models like Llama 3.1)
HF_TOKEN=

# HF repo for pushing/pulling LoRA adapters
HF_REPO_ID=

# ==================== Training Configuration (Vast.ai) ====================
# Base model for fine-tuning
BASE_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# Quantization bits (4 or 8)
QUANTIZATION_BITS=4

# LoRA hyperparameters
LORA_R=64
LORA_ALPHA=128
LORA_DROPOUT=0.05

# Training settings
TRAIN_VAL_SPLIT=0.8
MAX_SEQ_LENGTH=2048
LEARNING_RATE=2e-4
WARMUP_RATIO=0.03
GRADIENT_ACCUMULATION_STEPS=4
